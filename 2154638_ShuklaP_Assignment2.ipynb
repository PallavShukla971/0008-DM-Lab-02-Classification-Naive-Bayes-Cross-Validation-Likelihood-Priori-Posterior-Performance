{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61c136d7",
      "metadata": {
        "id": "61c136d7"
      },
      "source": [
        "# CSCM35, CSLM35 Big Data and Data Mining\n",
        "### by Dr. Jingjing Deng"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d0dff04",
      "metadata": {
        "id": "8d0dff04"
      },
      "source": [
        "This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
        "\n",
        "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
        "\n",
        "Reference:\n",
        "- https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
        "\n",
        "## Classification with Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e0da84e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0da84e3",
        "outputId": "e399141c-1969-43cd-e1db-22fa9e7524e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from collections import defaultdict\n",
        "# importing Statistics module\n",
        "import statistics as stat\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "print(\"Dataset loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069f7d5c",
      "metadata": {
        "id": "069f7d5c"
      },
      "source": [
        "### Cross Validation\n",
        "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a2234b2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2234b2e",
        "outputId": "76280eba-168b-4fe9-b206-b3408ff10c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ],
      "source": [
        "print(iris.data.shape)\n",
        "print(iris.target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "e4ccea6f",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ccea6f",
        "outputId": "7ea12873-4f40-4160-fb53-0bcd84f114b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (120, 4) x_test shape: (30, 4)\n",
            "y_train shape: (120,) y_test shape: (30,)\n",
            "[[ 0 40]\n",
            " [ 1 40]\n",
            " [ 2 40]]\n",
            "[[ 0 10]\n",
            " [ 1 10]\n",
            " [ 2 10]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2, stratify = iris.target,random_state = 42)\n",
        "\n",
        "print(\"x_train shape:\",x_train.shape,\"x_test shape:\",x_test.shape)\n",
        "print(\"y_train shape:\",y_train.shape,\"y_test shape:\",y_test.shape)\n",
        "species , counts = np.unique(y_train, return_counts = True)\n",
        "print(np.asarray((species, counts)).T)\n",
        "species , counts = np.unique(y_test, return_counts = True)\n",
        "print(np.asarray((species, counts)).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaede2cc",
      "metadata": {
        "id": "eaede2cc"
      },
      "source": [
        "Scaling the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "6b838163",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b838163",
        "outputId": "09b01ef8-b3ed-43e9-d701-e87a5f65019f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is scaled via the Standard Scaler\n"
          ]
        }
      ],
      "source": [
        "stdscale = StandardScaler()\n",
        "x_train = stdscale.fit_transform(x_train)\n",
        "x_test = stdscale.fit_transform(x_test)\n",
        "print(\"Data is scaled via the Standard Scaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d22cde",
      "metadata": {
        "id": "85d22cde"
      },
      "source": [
        "The terminology used in the Bayesian method of probability is as follows:\n",
        "<br>A is called the proposition\n",
        "<br>B is called the evidence\n",
        "<br>P(A) is called the prior probability of proposition \n",
        "<br>P(B) is called the prior probability of evidence.\n",
        "<br>P(A|B) is called the posterior probability\n",
        "<br>P(B|A) is the likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c9857b",
      "metadata": {
        "id": "a8c9857b"
      },
      "source": [
        "Reference:\n",
        "- https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9\n",
        "- https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
        "- https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81c03279",
      "metadata": {
        "id": "81c03279"
      },
      "source": [
        "# Grouping the data\n",
        "Each class is mapped to individual samples beloging to that class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "1a15d09a",
      "metadata": {
        "id": "1a15d09a"
      },
      "outputs": [],
      "source": [
        "def group_class(data,target):\n",
        "    target_map = defaultdict(list)\n",
        "    for index in range(len(data)):\n",
        "        features = data[index]\n",
        "        if not features.any():\n",
        "            continue\n",
        "        x = target[index]\n",
        "        target_map[x].append(features)  # designating the last column as the class column\n",
        "    return target_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0f19d8bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f19d8bd",
        "outputId": "233be70e-14c3-4840-b4a6-c643a34de078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grouped into 3 classes: dict_keys([0, 2, 1])\n"
          ]
        }
      ],
      "source": [
        "group = group_class(x_train,y_train)\n",
        "print (\"Grouped into %s classes: %s\" % (len(group.keys()), group.keys()))\n",
        "# print(group[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59211e12",
      "metadata": {
        "id": "59211e12"
      },
      "source": [
        "Reference for mean and standard deviation and zip:\n",
        "- https://docs.python.org/3/library/statistics.html\n",
        "- https://www.geeksforgeeks.org/statistical-functions-python-set-1averages-measure-central-location/\n",
        "- https://realpython.com/python-zip-function/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df930176",
      "metadata": {
        "id": "df930176"
      },
      "source": [
        "# Summary\n",
        "Return the (mean, standard deviation) combination for each feature of the train_set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "1ada8355",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ada8355",
        "outputId": "cd2cd286-e5bc-4cd3-c32b-79be12190b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary function defined\n",
            "Feature Summary: [{'stdev': 1.0041928905068676, 'mean': -1.2065580016577352e-15}, {'stdev': 1.0041928905068678, 'mean': -1.9935442185925468e-15}, {'stdev': 1.0041928905068678, 'mean': 4.844504427244563e-16}, {'stdev': 1.0041928905068676, 'mean': 1.6581354345124311e-15}]\n"
          ]
        }
      ],
      "source": [
        "def summarize(test_set):\n",
        "    \"\"\"\n",
        "    Use zip to line up each feature into a single column across multiple lists.\n",
        "    yield the mean and the stdev for each feature.\n",
        "    \"\"\"\n",
        "    for feature in zip(*test_set):\n",
        "        yield {\n",
        "            'stdev': stat.stdev(feature),\n",
        "            'mean': stat.mean(feature)\n",
        "        }\n",
        "print(\"Summary function defined\")\n",
        "#usage: \n",
        "print (\"Feature Summary: %s\" % [i for i in summarize(x_train)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7df2d1",
      "metadata": {
        "id": "fd7df2d1"
      },
      "source": [
        "# Building the model\n",
        "Features and class<br>\n",
        "Features: \n",
        "- Sepal length sl\n",
        "- Sepal width sw\n",
        "- Petal length pl\n",
        "- Petal width pw\n",
        "<br><br>\n",
        "Class:\n",
        "- Setosa s\n",
        "- Versicolor ve\n",
        "- Virginicas vi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a5481f",
      "metadata": {
        "id": "e8a5481f"
      },
      "source": [
        "## Posterior = (Class Prior*Likelihood)/(Predictor Prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "805c364a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "805c364a",
        "outputId": "b1c6583f-506c-4eba-8afb-5e509e03746a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(0): 0.3333333333333333\n",
            "P(1): 0.3333333333333333\n",
            "P(2): 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "#probabilities of individual categories\n",
        "#Class Prior\n",
        "def prior_prob(group, target, data):\n",
        "    total = float(len(data))\n",
        "    result = len(group[target]) / total\n",
        "    return result\n",
        "\n",
        "for target_class in [0, 1, 2]:\n",
        "    prior_probcalled = prior_prob(group, target_class, x_train)\n",
        "    print('P(%s): %s' % (target_class, prior_probcalled))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb554b24",
      "metadata": {
        "id": "bb554b24"
      },
      "source": [
        "# Train the model\n",
        "we calculate the mean and standard deviation to learn from the train set. Using the above grouped classes, the combination (mean, standard deviation) for each feature of each class is calculated.\n",
        "\n",
        "This will be used to calculate class Likelihoods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd3bf155",
      "metadata": {
        "id": "cd3bf155"
      },
      "source": [
        "Reference: \n",
        "- https://www.geeksforgeeks.org/difference-between-dict-items-and-dict-iteritems-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "9af755a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af755a9",
        "outputId": "a1fcac2f-9afa-483e-e1e2-eb03851d5c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: {'prior_probcalled': 0.3333333333333333, 'Summary': [{'stdev': 0.3683599758072141, 'mean': -1.0229893930683729}, {'stdev': 0.8934701230479329, 'mean': 0.820924097042234}, {'stdev': 0.09136539442393624, 'mean': -1.301716621475558}, {'stdev': 0.1518922489233574, 'mean': -1.2508575153646249}]}, 2: {'prior_probcalled': 0.3333333333333333, 'Summary': [{'stdev': 0.8179090891738221, 'mean': 0.9175060509815539}, {'stdev': 0.7922953328073759, 'mean': -0.1529903999033278}, {'stdev': 0.32712147411454845, 'mean': 1.0277457294965155}, {'stdev': 0.3538084970102825, 'mean': 1.0994379213994365}]}, 1: {'prior_probcalled': 0.3333333333333333, 'Summary': [{'stdev': 0.5693008500693436, 'mean': 0.1054833420868153}, {'stdev': 0.6914658560795356, 'mean': -0.6679336971389123}, {'stdev': 0.25164140430617404, 'mean': 0.2739708919790439}, {'stdev': 0.24624029261503796, 'mean': 0.1514195939651933}]}}\n"
          ]
        }
      ],
      "source": [
        "def train(train_list, target):\n",
        "    '''\n",
        "    For each target:\n",
        "        1. yield prior_prob: the probability of each class. P(class) eg P(Iris-virginica)\n",
        "        2. yield summary: list of {'mean': 0.0, 'stdev': 0.0}\n",
        "    '''\n",
        "    group = group_class(train_list, target)\n",
        "    summaries = {}\n",
        "    for target, features in group.items():\n",
        "        summaries[target] = {\n",
        "            'prior_probcalled': prior_prob(group, target, train_list),\n",
        "            'Summary': [i for i in summarize(features)],\n",
        "        }\n",
        "    return summaries\n",
        "\n",
        "summaries = train(x_train, y_train)\n",
        "print(summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dca49e2",
      "metadata": {
        "id": "0dca49e2"
      },
      "source": [
        "# Likelihood\n",
        "product of all normal probabilities (for each feature given the class) P(sl|s)xP(sw|s)xP(pl|s)xP(pw|s)\n",
        "\n",
        "Reference:\n",
        "- https://en.wikipedia.org/wiki/Normal_distribution\n",
        "\n",
        "Once we have the likelihood function we'll use the same to calculate the <b>joint probablities</b> as a product of Prior Probability and the Likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f544332",
      "metadata": {
        "id": "9f544332"
      },
      "source": [
        "Reference:\n",
        "- https://stackoverflow.com/questions/43602270/what-is-probability-density-function-in-the-context-of-scipy-stats-norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "8f1e155c",
      "metadata": {
        "id": "8f1e155c"
      },
      "outputs": [],
      "source": [
        "def normal_pdf(x,mean,stdev):\n",
        "    return stats.norm(mean,stdev).pdf(x)\n",
        "\n",
        "def get_prediction(test_vector):\n",
        "    '''\n",
        "    :param test_vector: single list of features to test\n",
        "    :return:\n",
        "    Return the target class with the largest/best posterior probability\n",
        "    '''\n",
        "    posterior_probs = posterior_probabilities(test_vector)\n",
        "    best_target = max(posterior_probs, key=posterior_probs.get)\n",
        "    return best_target\n",
        "    \n",
        "def joint_probabilities(test_row):\n",
        "    '''\n",
        "    :param test_row: single list of features to test; new data\n",
        "    :return:\n",
        "    Use the normal_pdf(x, mean, stdev) to calculate the Normal Probability for each feature\n",
        "    Take the product of all Normal Probabilities and the Prior Probability.\n",
        "    '''\n",
        "    joint_probs = {}\n",
        "    for target, features in summaries.items():\n",
        "        total_features = len(features['Summary'])\n",
        "        likelihood = 1\n",
        "        for index in range(total_features):\n",
        "            feature = test_row[index]\n",
        "            mean = features['Summary'][index]['mean']\n",
        "            stdev = features['Summary'][index]['stdev']\n",
        "            normal_prob = normal_pdf(feature, mean, stdev)\n",
        "            likelihood *= normal_prob\n",
        "        prior_prob = features['prior_probcalled']\n",
        "        joint_probs[target] = prior_prob * likelihood\n",
        "    return joint_probs\n",
        "\n",
        "def posterior_probabilities(test_row):\n",
        "    '''\n",
        "    :param test_row: single list of features to test; new data\n",
        "    :return:\n",
        "    For each feature (x) in the test_row:\n",
        "        1. Calculate Predictor Prior Probability using the Normal PDF N(x; µ, σ). eg = P(feature | class)\n",
        "        2. Calculate Likelihood by getting the product of the prior and the Normal PDFs\n",
        "        3. Multiply Likelihood by the prior to calculate the Joint Probability.\n",
        "    E.g.\n",
        "    prior_prob: P(setosa)\n",
        "    likelihood: P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)\n",
        "    joint_prob: prior_prob * likelihood\n",
        "    marginal_prob: predictor prior probability\n",
        "    posterior_prob = joint_prob/ marginal_prob\n",
        "    returning a dictionary mapping of class to it's posterior probability\n",
        "    '''\n",
        "    posterior_probs = {}\n",
        "    joint_probabilities1 = joint_probabilities(test_row)\n",
        "    marginal_prob = marginal_pdf(joint_probabilities1)\n",
        "    for target, joint_prob in joint_probabilities1.items():\n",
        "        posterior_probs[target] = joint_prob / marginal_prob\n",
        "    return posterior_probs\n",
        "\n",
        "def marginal_pdf(joint_probabilities1):\n",
        "    '''\n",
        "    :param joint_probabilities: list of joint probabilities for each feature\n",
        "    :return:\n",
        "    Marginal Probability Density Function (Predictor Prior Probability)\n",
        "    Joint Probability = prior * likelihood\n",
        "    Marginal Probability is the sum of all joint probabilities for all classes.\n",
        "    marginal_pdf =\n",
        "      [P(setosa) * P(sepal length | setosa) * P(sepal width | setosa) * P(petal length | setosa) * P(petal width | setosa)]\n",
        "    + [P(versicolour) * P(sepal length | versicolour) * P(sepal width | versicolour) * P(petal length | versicolour) * P(petal width | versicolour)]\n",
        "    + [P(virginica) * P(sepal length | verginica) * P(sepal width | verginica) * P(petal length | verginica) * P(petal width | verginica)]\n",
        "    '''\n",
        "    marginal_prob = sum(joint_probabilities1.values())\n",
        "    return marginal_prob\n",
        "\n",
        "def predict(test_set):\n",
        "    '''\n",
        "    Predict the likeliest target for each row of the test_set.\n",
        "    Return a list of predicted targets.\n",
        "    '''\n",
        "    predictions = []\n",
        "    for row in test_set:\n",
        "        result = get_prediction(row)\n",
        "        predictions.append(result)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def accuracy(test_set, predicted):\n",
        "    '''\n",
        "    :param test_set: list of test_data\n",
        "    :param predicted: list of predicted classes\n",
        "    :return:\n",
        "    Calculate the the average performance of the classifier.\n",
        "    '''\n",
        "    correct = 0\n",
        "#     actual = [item[-1] for item in test_set]\n",
        "    actual = test_set\n",
        "    for x, y in zip(actual, predicted):\n",
        "        if x == y:\n",
        "            correct += 1\n",
        "    return correct / float(len(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "c48b8580",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48b8580",
        "outputId": "c4c8a213-4db5-42e1-fae3-49f3e27a0802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 1, 1, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 2, 1, 0, 2, 0]\n",
            "[0 2 1 1 0 1 0 0 2 1 2 2 2 1 0 0 0 1 1 2 0 2 1 2 2 1 1 0 2 0]\n",
            "type of predicated_list: <class 'list'>\n",
            "Type of y_test: <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# INCORRECT - was trying to compare list with numpy array\n",
        "print(predicted_list)\n",
        "print(y_test)\n",
        "# accuracy = accuracy(y_test, predicted_list)\n",
        "# print('Accuracy: %.3f' % accuracy)\n",
        "print(\"type of predicated_list:\",type(predicted_list))\n",
        "print(\"Type of y_test:\",type(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "3ca78cdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ca78cdd",
        "outputId": "a039f231-5cd6-43ef-84c7-a52450154618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n"
          ]
        }
      ],
      "source": [
        "print(y_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "f5db9c1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5db9c1a",
        "outputId": "d2755015-8b70-448c-fb25-cd87161d4bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 93.333\n"
          ]
        }
      ],
      "source": [
        "predicted_list = predict(x_test)\n",
        "accuracy = accuracy(y_test.tolist(), predicted_list)\n",
        "print('Accuracy: %.3f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REFERENCES: \n",
        "\n",
        "https://stackoverflow.com/questions/68799909/classification-accuracy-with-sklearn-in-percentage "
      ],
      "metadata": {
        "id": "VTOYm3fG7_eZ"
      },
      "id": "VTOYm3fG7_eZ"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MZutLEyu8BMS"
      },
      "id": "MZutLEyu8BMS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "2154638-ShuklaP-Assignment2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}